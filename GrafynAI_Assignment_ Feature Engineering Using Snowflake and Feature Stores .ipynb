{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede493e-6126-4df8-bbca-506bc4c8fc40",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- PHASE 1: Environment Setup and Raw Data Creation\n",
    "-- ============================================================================\n",
    "\n",
    "CREATE OR REPLACE WAREHOUSE fe_wh \n",
    "  WITH WAREHOUSE_SIZE = 'XSMALL' \n",
    "  AUTO_SUSPEND = 60 \n",
    "  AUTO_RESUME = TRUE\n",
    "  INITIALLY_SUSPENDED = TRUE;\n",
    "\n",
    "USE WAREHOUSE fe_wh;\n",
    "\n",
    "CREATE OR REPLACE DATABASE fe_demo_db;\n",
    "USE DATABASE fe_demo_db;\n",
    "\n",
    "CREATE OR REPLACE SCHEMA fe_raw;\n",
    "CREATE OR REPLACE SCHEMA fe_features;\n",
    "CREATE OR REPLACE SCHEMA fe_store;\n",
    "\n",
    "SELECT 'Environment setup completed!' AS status;\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_raw.customer_events (\n",
    "  customer_id STRING,\n",
    "  event_ts    TIMESTAMP_NTZ,\n",
    "  event_type  STRING,\n",
    "  channel     STRING,\n",
    "  amount      NUMBER(12,2)\n",
    ");\n",
    "\n",
    "SELECT 'Raw table created!' AS status;\n",
    "\n",
    "INSERT INTO fe_raw.customer_events VALUES\n",
    "  ('C001', '2024-08-01 10:00:00', 'purchase', 'web',   120.50),\n",
    "  ('C001', '2024-08-03 09:15:00', 'browse',   'web',     0.00),\n",
    "  ('C001', '2024-08-05 14:20:00', 'purchase', 'mobile',  75.10),\n",
    "  ('C001', '2024-08-07 11:30:00', 'purchase', 'web',    95.00),\n",
    "  ('C001', '2024-08-10 08:00:00', 'browse',   'mobile',   0.00),\n",
    "  ('C001', '2024-08-15 16:45:00', 'purchase', 'web',   180.00),\n",
    "  ('C001', '2024-09-01 10:00:00', 'purchase', 'web',   150.00),\n",
    "  ('C001', '2024-09-08 15:00:00', 'purchase', 'web',   200.00),\n",
    "  \n",
    "  ('C002', '2024-08-02 11:05:00', 'purchase', 'web',   220.00),\n",
    "  ('C002', '2024-08-04 16:45:00', 'refund',   'web',   -50.00),\n",
    "  ('C002', '2024-08-06 10:20:00', 'purchase', 'mobile', 150.00),\n",
    "  ('C002', '2024-08-12 14:30:00', 'purchase', 'web',   300.00),\n",
    "  ('C002', '2024-08-20 09:00:00', 'purchase', 'mobile',  95.00),\n",
    "  ('C002', '2024-09-02 11:00:00', 'purchase', 'web',   250.00),\n",
    "  \n",
    "  ('C003', '2024-08-01 08:00:00', 'browse',   'mobile',   0.00),\n",
    "  ('C003', '2024-08-03 12:00:00', 'browse',   'web',      0.00),\n",
    "  ('C003', '2024-08-06 12:30:00', 'purchase', 'web',    180.00),\n",
    "  ('C003', '2024-08-15 10:00:00', 'browse',   'mobile',   0.00),\n",
    "  ('C003', '2024-08-25 14:00:00', 'browse',   'web',      0.00),\n",
    "  ('C003', '2024-09-05 11:00:00', 'browse',   'web',      0.00),\n",
    "  \n",
    "  ('C004', '2024-08-02 14:00:00', 'purchase', 'web',    300.00),\n",
    "  ('C004', '2024-08-05 09:00:00', 'purchase', 'mobile',  50.00),\n",
    "  ('C004', '2024-08-12 15:30:00', 'purchase', 'web',    450.00),\n",
    "  ('C004', '2024-08-20 10:00:00', 'purchase', 'web',    280.00),\n",
    "  ('C004', '2024-09-03 13:00:00', 'purchase', 'mobile', 320.00),\n",
    "  \n",
    "  ('C005', '2024-08-01 10:00:00', 'browse',   'web',      0.00),\n",
    "  ('C005', '2024-08-02 15:00:00', 'purchase', 'web',    200.00),\n",
    "  ('C005', '2024-08-03 09:00:00', 'browse',   'mobile',   0.00),\n",
    "  \n",
    "  ('C006', '2024-08-04 11:00:00', 'purchase', 'web',    175.00),\n",
    "  ('C006', '2024-08-08 14:00:00', 'purchase', 'mobile',  95.00),\n",
    "  ('C006', '2024-08-15 10:00:00', 'purchase', 'web',    210.00),\n",
    "  ('C006', '2024-08-22 16:00:00', 'purchase', 'web',    135.00),\n",
    "  ('C006', '2024-09-05 12:00:00', 'purchase', 'mobile', 180.00),\n",
    "  \n",
    "  ('C007', '2024-08-05 09:00:00', 'browse',   'web',      0.00),\n",
    "  ('C007', '2024-08-06 14:00:00', 'purchase', 'web',    250.00),\n",
    "  \n",
    "  ('C008', '2024-08-06 10:00:00', 'purchase', 'web',    160.00),\n",
    "  ('C008', '2024-08-10 11:00:00', 'purchase', 'mobile', 120.00),\n",
    "  ('C008', '2024-08-18 15:00:00', 'purchase', 'web',    190.00),\n",
    "  ('C008', '2024-08-27 09:00:00', 'purchase', 'web',    140.00),\n",
    "  ('C008', '2024-09-06 14:00:00', 'purchase', 'mobile', 170.00),\n",
    "  \n",
    "  ('C009', '2024-08-07 12:00:00', 'purchase', 'web',    200.00),\n",
    "  ('C009', '2024-08-09 10:00:00', 'browse',   'mobile',   0.00),\n",
    "  ('C009', '2024-08-14 15:00:00', 'purchase', 'web',    280.00),\n",
    "  ('C009', '2024-08-21 11:00:00', 'purchase', 'mobile', 220.00),\n",
    "  ('C009', '2024-09-01 14:00:00', 'purchase', 'web',    310.00),\n",
    "  ('C009', '2024-09-07 10:00:00', 'purchase', 'mobile', 195.00),\n",
    "  \n",
    "  ('C010', '2024-08-08 09:00:00', 'browse',   'web',      0.00),\n",
    "  ('C010', '2024-08-09 15:00:00', 'purchase', 'web',    185.00),\n",
    "  \n",
    "  ('C011', '2024-08-10 10:00:00', 'purchase', 'web',    145.00),\n",
    "  ('C011', '2024-08-15 14:00:00', 'purchase', 'mobile', 175.00),\n",
    "  ('C011', '2024-08-22 11:00:00', 'purchase', 'web',    195.00),\n",
    "  ('C011', '2024-09-02 09:00:00', 'purchase', 'web',    160.00),\n",
    "  ('C011', '2024-09-09 15:00:00', 'purchase', 'mobile', 200.00);\n",
    "\n",
    "SELECT COUNT(*) AS total_records FROM fe_raw.customer_events;\n",
    "SELECT COUNT(DISTINCT customer_id) AS unique_customers FROM fe_raw.customer_events;\n",
    "\n",
    "SELECT * FROM fe_raw.customer_events ORDER BY customer_id, event_ts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ef909-68e0-48f3-9560-a2e19096806e",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- PHASE 2: Feature Engineering\n",
    "-- ============================================================================\n",
    "\n",
    "USE WAREHOUSE fe_wh;\n",
    "USE DATABASE fe_demo_db;\n",
    "USE SCHEMA fe_features;\n",
    "\n",
    "CREATE OR REPLACE VIEW fe_features.customer_features_v AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    customer_id,\n",
    "    event_ts,\n",
    "    event_type,\n",
    "    channel,\n",
    "    amount\n",
    "  FROM fe_raw.customer_events\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    customer_id,\n",
    "    MAX(event_ts) AS last_event_ts,\n",
    "    DATEDIFF('day', MAX(event_ts), CURRENT_TIMESTAMP()) AS days_since_last_event,\n",
    "    COUNT_IF(event_type='purchase') AS purchases_7d,\n",
    "    SUM(CASE WHEN event_type='purchase' THEN amount ELSE 0 END) AS revenue_7d,\n",
    "    COUNT_IF(event_type='refund') AS refunds_7d,\n",
    "    SUM(CASE WHEN event_type='refund' THEN amount ELSE 0 END) AS refund_amount_7d,\n",
    "    COUNT(*) AS events_7d,\n",
    "    COUNT_IF(channel='mobile') AS mobile_events_7d,\n",
    "    COUNT_IF(channel='web') AS web_events_7d,\n",
    "    AVG(CASE WHEN amount<>0 THEN amount END) AS avg_amount_nonzero_7d,\n",
    "    COUNT_IF(event_type='browse') AS browse_events_7d,\n",
    "    STDDEV(amount) AS amount_stddev_7d,\n",
    "    MAX(amount) AS max_amount_7d,\n",
    "    MIN(amount) AS min_amount_7d\n",
    "  FROM base\n",
    "  GROUP BY customer_id\n",
    ")\n",
    "SELECT * FROM agg;\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_features.customer_features AS\n",
    "SELECT \n",
    "  *,\n",
    "  CURRENT_TIMESTAMP() AS feature_timestamp\n",
    "FROM fe_features.customer_features_v;\n",
    "\n",
    "SELECT * FROM fe_features.customer_features ORDER BY customer_id;\n",
    "\n",
    "CREATE OR REPLACE VIEW fe_features.customer_features_online AS\n",
    "SELECT *\n",
    "FROM fe_features.customer_features\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY feature_timestamp DESC) = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab21cc-b0d3-4200-a479-f7d25f987367",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- PHASE 3: Feature Store Setup (SQL-Only Pattern)\n",
    "-- Use this if Snowpark ML Feature Store is not available\n",
    "-- ============================================================================\n",
    "\n",
    "USE WAREHOUSE fe_wh;\n",
    "USE DATABASE fe_demo_db;\n",
    "USE SCHEMA fe_store;\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_store.entity_registry (\n",
    "    entity_name STRING,\n",
    "    identifier_column STRING,\n",
    "    description STRING\n",
    ");\n",
    "\n",
    "INSERT INTO fe_store.entity_registry VALUES\n",
    "    ('customer', 'customer_id', 'Customer entity for churn prediction');\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_store.feature_view_registry (\n",
    "    feature_view_name STRING,\n",
    "    entity_name STRING,\n",
    "    description STRING,\n",
    "    features ARRAY\n",
    ");\n",
    "\n",
    "INSERT INTO fe_store.feature_view_registry VALUES\n",
    "    ('customer_features', 'customer', 'Customer behavioral features from 7-day window', \n",
    "     ARRAY_CONSTRUCT('purchases_7d', 'revenue_7d', 'days_since_last_event', 'events_7d'));\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_store.customer_features_online (\n",
    "    customer_id STRING,\n",
    "    days_since_last_event NUMBER,\n",
    "    purchases_7d NUMBER,\n",
    "    revenue_7d NUMBER,\n",
    "    refunds_7d NUMBER,\n",
    "    refund_amount_7d NUMBER,\n",
    "    events_7d NUMBER,\n",
    "    mobile_events_7d NUMBER,\n",
    "    web_events_7d NUMBER,\n",
    "    avg_amount_nonzero_7d NUMBER,\n",
    "    browse_events_7d NUMBER,\n",
    "    amount_stddev_7d NUMBER,\n",
    "    max_amount_7d NUMBER,\n",
    "    min_amount_7d NUMBER,\n",
    "    feature_timestamp TIMESTAMP_NTZ\n",
    ");\n",
    "\n",
    "INSERT INTO fe_store.customer_features_online\n",
    "SELECT * FROM fe_features.customer_features_online;\n",
    "\n",
    "SELECT * FROM fe_store.entity_registry;\n",
    "SELECT * FROM fe_store.feature_view_registry;\n",
    "SELECT * FROM fe_store.customer_features_online LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdbe03-75f9-45be-bd84-657d9105d844",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- PHASE 4: Create Training Labels and Offline Feature Retrieval\n",
    "-- ============================================================================\n",
    "\n",
    "USE WAREHOUSE fe_wh;\n",
    "USE DATABASE fe_demo_db;\n",
    "USE SCHEMA fe_raw;\n",
    "\n",
    "DELETE FROM fe_features.customer_features;\n",
    "INSERT INTO fe_features.customer_features\n",
    "SELECT *, CURRENT_TIMESTAMP() AS feature_timestamp\n",
    "FROM fe_features.customer_features_v;\n",
    "\n",
    "CREATE OR REPLACE VIEW fe_features.customer_features_online AS\n",
    "SELECT *\n",
    "FROM fe_features.customer_features\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY feature_timestamp DESC) = 1;\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_raw.labels (\n",
    "  customer_id STRING,\n",
    "  label_ts    TIMESTAMP_NTZ,\n",
    "  churned     BOOLEAN\n",
    ");\n",
    "\n",
    "INSERT INTO fe_raw.labels VALUES\n",
    "  ('C001', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C002', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C003', CURRENT_TIMESTAMP(), TRUE),\n",
    "  ('C004', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C005', CURRENT_TIMESTAMP(), TRUE),\n",
    "  ('C006', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C007', CURRENT_TIMESTAMP(), TRUE),\n",
    "  ('C008', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C009', CURRENT_TIMESTAMP(), FALSE),\n",
    "  ('C010', CURRENT_TIMESTAMP(), TRUE),\n",
    "  ('C011', CURRENT_TIMESTAMP(), FALSE);\n",
    "\n",
    "SELECT * FROM fe_raw.labels;\n",
    "\n",
    "SELECT \n",
    "    'Features Check' AS check_type,\n",
    "    COUNT(*) AS feature_count,\n",
    "    MIN(feature_timestamp) AS min_feature_ts,\n",
    "    MAX(feature_timestamp) AS max_feature_ts\n",
    "FROM fe_features.customer_features;\n",
    "\n",
    "SELECT \n",
    "    'Labels Check' AS check_type,\n",
    "    COUNT(*) AS label_count,\n",
    "    MIN(label_ts) AS min_label_ts,\n",
    "    MAX(label_ts) AS max_label_ts\n",
    "FROM fe_raw.labels;\n",
    "\n",
    "CREATE OR REPLACE VIEW fe_features.training_set AS\n",
    "WITH ranked AS (\n",
    "  SELECT \n",
    "    f.*,\n",
    "    l.label_ts,\n",
    "    l.churned,\n",
    "    ROW_NUMBER() OVER (\n",
    "      PARTITION BY l.customer_id, l.label_ts\n",
    "      ORDER BY f.feature_timestamp DESC\n",
    "    ) AS rn\n",
    "  FROM fe_features.customer_features f\n",
    "  JOIN fe_raw.labels l\n",
    "    ON f.customer_id = l.customer_id\n",
    "   AND f.feature_timestamp <= l.label_ts\n",
    ")\n",
    "SELECT \n",
    "  customer_id,\n",
    "  days_since_last_event,\n",
    "  purchases_7d,\n",
    "  revenue_7d,\n",
    "  refunds_7d,\n",
    "  refund_amount_7d,\n",
    "  events_7d,\n",
    "  mobile_events_7d,\n",
    "  web_events_7d,\n",
    "  avg_amount_nonzero_7d,\n",
    "  browse_events_7d,\n",
    "  amount_stddev_7d,\n",
    "  max_amount_7d,\n",
    "  min_amount_7d,\n",
    "  churned AS label\n",
    "FROM ranked\n",
    "WHERE rn = 1;\n",
    "\n",
    "SELECT * FROM fe_features.training_set ORDER BY customer_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c7bde-db80-43d0-8696-beee15419728",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 5: Model Training Using Engineered Features\n",
    "# Run this in a Snowflake Python Worksheet\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PACKAGE INSTALLATION CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâš ï¸  BEFORE RUNNING THIS CODE:\")\n",
    "print(\"   1. Look for 'Packages' button at the TOP of your notebook\")\n",
    "print(\"   2. Click 'Packages' â†’ Add these packages:\")\n",
    "print(\"      â€¢ scikit-learn\")\n",
    "print(\"      â€¢ pandas\")\n",
    "print(\"      â€¢ numpy\")\n",
    "print(\"   3. Click 'Save' or 'Apply'\")\n",
    "print(\"   4. Wait 1-2 minutes for installation\")\n",
    "print(\"   5. Then re-run this cell\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "packages_ok = True\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"âœ… pandas: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ pandas: NOT FOUND - {e}\")\n",
    "    packages_ok = False\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"âœ… numpy: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ numpy: NOT FOUND - {e}\")\n",
    "    packages_ok = False\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        auc,\n",
    "        classification_report,\n",
    "        confusion_matrix\n",
    "    )\n",
    "    print(\"âœ… scikit-learn: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ scikit-learn: NOT FOUND - {e}\")\n",
    "    packages_ok = False\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"âœ… matplotlib: OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ matplotlib: NOT FOUND - {e}\")\n",
    "    packages_ok = False\n",
    "\n",
    "if not packages_ok:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âŒ REQUIRED PACKAGES NOT INSTALLED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nðŸ“¦ INSTALLATION STEPS:\")\n",
    "    print(\"   1. Find 'Packages' button at the TOP of your Snowflake notebook\")\n",
    "    print(\"      (Usually in the toolbar, may have a ðŸ“¦ icon)\")\n",
    "    print(\"   2. Click 'Packages' button\")\n",
    "    print(\"   3. Click 'Add Package' or '+' button\")\n",
    "    print(\"   4. Type and add each package:\")\n",
    "    print(\"      â†’ scikit-learn\")\n",
    "    print(\"      â†’ pandas\")\n",
    "    print(\"      â†’ numpy\")\n",
    "    print(\"   5. Click 'Save' or 'Apply'\")\n",
    "    print(\"   6. Wait 1-2 minutes for packages to install\")\n",
    "    print(\"   7. Re-run this cell\")\n",
    "    print(\"\\nðŸ’¡ TIP: If you can't find the Packages button:\")\n",
    "    print(\"   â€¢ Make sure you're in a Notebook (not SQL Worksheet)\")\n",
    "    print(\"   â€¢ Try refreshing the page\")\n",
    "    print(\"   â€¢ Check the top toolbar/menu\")\n",
    "    print(\"\\nðŸ“– For detailed guide, see: INSTALL_PACKAGES_GUIDE.md\")\n",
    "    print(\"=\"*70)\n",
    "    raise ImportError(\n",
    "        \"Required packages not installed. \"\n",
    "        \"Please install scikit-learn, pandas, and numpy using the Packages button, then re-run this cell.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nâœ… All required packages are installed!\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "try:\n",
    "    _ = session\n",
    "    print(\"âœ… Using session from notebook context\")\n",
    "except NameError:\n",
    "    try:\n",
    "        import snowflake.snowpark.context as snowpark_context\n",
    "        session = snowpark_context.get_active_session()\n",
    "        print(\"âœ… Got session from snowpark context\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not get session: {e}\")\n",
    "        print(\"Please ensure you're running this in a Snowflake notebook\")\n",
    "        raise\n",
    "\n",
    "session.use_warehouse(\"FE_WH\")\n",
    "session.use_database(\"FE_DEMO_DB\")\n",
    "session.use_schema(\"FE_FEATURES\")\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    training_df = session.sql(\"\"\"\n",
    "        SELECT * FROM fe_features.training_set\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    training_df.columns = [c.lower() for c in training_df.columns]\n",
    "    \n",
    "    if training_df.empty:\n",
    "        raise ValueError(\"Training set is empty! Please run Phase 4 (04_training_set_creation.sql) first.\")\n",
    "    \n",
    "    print(f\"âœ… Training set loaded successfully\")\n",
    "    print(f\"Training set shape: {training_df.shape}\")\n",
    "    print(f\"\\nTraining set preview:\")\n",
    "    print(training_df.head())\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(training_df['label'].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading training data: {e}\")\n",
    "    print(\"\\nðŸ’¡ SOLUTION:\")\n",
    "    print(\"   1. Make sure you've run Phase 4 (04_training_set_creation.sql)\")\n",
    "    print(\"   2. Verify the training_set view exists: SELECT * FROM fe_features.training_set\")\n",
    "    raise\n",
    "\n",
    "feature_columns = [\n",
    "    'days_since_last_event',\n",
    "    'purchases_7d',\n",
    "    'revenue_7d',\n",
    "    'refunds_7d',\n",
    "    'refund_amount_7d',\n",
    "    'events_7d',\n",
    "    'mobile_events_7d',\n",
    "    'web_events_7d',\n",
    "    'avg_amount_nonzero_7d',\n",
    "    'browse_events_7d',\n",
    "    'amount_stddev_7d',\n",
    "    'max_amount_7d',\n",
    "    'min_amount_7d'\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in feature_columns if col not in training_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"âš ï¸  Warning: Missing columns: {missing_columns}\")\n",
    "    print(f\"Available columns: {list(training_df.columns)}\")\n",
    "    feature_columns = [col for col in feature_columns if col in training_df.columns]\n",
    "    print(f\"Using available columns: {feature_columns}\")\n",
    "\n",
    "if 'label' not in training_df.columns:\n",
    "    raise ValueError(\"'label' column not found in training set! Please check Phase 4.\")\n",
    "\n",
    "X = training_df[feature_columns].fillna(0)\n",
    "y = training_df['label'].astype(int)\n",
    "\n",
    "if len(y.unique()) < 2:\n",
    "    print(f\"âš ï¸  Warning: Only one class found in labels: {y.unique()}\")\n",
    "    print(\"Model training will proceed, but evaluation metrics may be limited.\")\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPLITTING DATA AND TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(X) >= 20:\n",
    "    print(f\"âœ… Dataset size: {len(X)} samples. Using stratified 70/30 split.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "elif len(X) >= 10:\n",
    "    print(f\"âœ… Dataset size: {len(X)} samples. Using stratified 80/20 split.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(f\"âš ï¸  Small dataset ({len(X)} samples). Using 80/20 split without stratification.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Training Logistic Regression model...\")\n",
    "print(\"-\"*60)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=min(5, len(X)//2), shuffle=True, random_state=42)\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "print(\"\\n=== Logistic Regression Results ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"Cross-validation F1 (mean Â± std): {cv_scores_lr.mean():.4f} Â± {cv_scores_lr.std():.4f}\")\n",
    "\n",
    "try:\n",
    "    if len(set(y_test)) > 1 and len(set(y_pred_proba_lr)) > 1:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "        print(f\"ROC AUC: {auc_score:.4f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (requires both classes in test set)\")\n",
    "except Exception as e:\n",
    "    print(f\"ROC AUC: N/A ({e})\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Training Random Forest model...\")\n",
    "print(\"-\"*60)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "print(\"\\n=== Random Forest Results ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"Cross-validation F1 (mean Â± std): {cv_scores_rf.mean():.4f} Â± {cv_scores_rf.std():.4f}\")\n",
    "\n",
    "try:\n",
    "    if len(set(y_test)) > 1 and len(set(y_pred_proba_rf)) > 1:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "        print(f\"ROC AUC: {auc_score:.4f}\")\n",
    "    else:\n",
    "        print(\"ROC AUC: N/A (requires both classes in test set)\")\n",
    "except Exception as e:\n",
    "    print(f\"ROC AUC: N/A ({e})\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== Feature Importance (Random Forest) ===\")\n",
    "print(feature_importance)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Creating model evaluation visualizations...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Model Evaluation Metrics and Visualizations', fontsize=16, fontweight='bold')\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['No Churn', 'Churn'])\n",
    "disp_lr.plot(ax=axes[0, 0], cmap='Blues')\n",
    "axes[0, 0].set_title('Confusion Matrix - Logistic Regression')\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['No Churn', 'Churn'])\n",
    "disp_rf.plot(ax=axes[0, 1], cmap='Greens')\n",
    "axes[0, 1].set_title('Confusion Matrix - Random Forest')\n",
    "\n",
    "axes[0, 2].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "if len(set(y_test)) > 1 and len(set(y_pred_proba_lr)) > 1:\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    axes[0, 2].plot(fpr_lr, tpr_lr, 'b-', label=f'LR (AUC={roc_auc_lr:.3f})', linewidth=2)\n",
    "if len(set(y_test)) > 1 and len(set(y_pred_proba_rf)) > 1:\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "    roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "    axes[0, 2].plot(fpr_rf, tpr_rf, 'g-', label=f'RF (AUC={roc_auc_rf:.3f})', linewidth=2)\n",
    "axes[0, 2].set_xlabel('False Positive Rate')\n",
    "axes[0, 2].set_ylabel('True Positive Rate')\n",
    "axes[0, 2].set_title('ROC Curves Comparison')\n",
    "axes[0, 2].legend(loc='lower right')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "top_features = feature_importance.head(10)\n",
    "axes[1, 0].barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "axes[1, 0].set_yticks(range(len(top_features)))\n",
    "axes[1, 0].set_yticklabels(top_features['feature'].values)\n",
    "axes[1, 0].set_xlabel('Importance')\n",
    "axes[1, 0].set_title('Top 10 Feature Importance (Random Forest)')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "cv_data = pd.DataFrame({\n",
    "    'Logistic Regression': cv_scores_lr,\n",
    "    'Random Forest': cv_scores_rf\n",
    "})\n",
    "axes[1, 1].boxplot(cv_data.values, labels=cv_data.columns)\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('Cross-Validation F1 Scores Distribution')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "accuracy_scores = [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_rf)]\n",
    "f1_scores = [f1_score(y_test, y_pred_lr, zero_division=0), f1_score(y_test, y_pred_rf, zero_division=0)]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "axes[1, 2].bar(x - width/2, accuracy_scores, width, label='Accuracy', color='skyblue')\n",
    "axes[1, 2].bar(x + width/2, f1_scores, width, label='F1 Score', color='orange')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Model Performance Comparison')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(models, rotation=15, ha='right')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].set_ylim([0, 1])\n",
    "axes[1, 2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "try:\n",
    "    import io\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    stage_name = \"fe_features.model_stage\"\n",
    "    session.sql(f\"CREATE OR REPLACE STAGE {stage_name}\").collect()\n",
    "    \n",
    "    temp_img_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
    "    with open(temp_img_path, 'wb') as f:\n",
    "        f.write(buffer.read())\n",
    "    \n",
    "    session.file.put(\n",
    "        local_file_name=temp_img_path,\n",
    "        stage_location=f\"@{stage_name}\",\n",
    "        overwrite=True,\n",
    "        auto_compress=False\n",
    "    )\n",
    "    print(f\"âœ… Visualization saved to stage: {stage_name}/model_evaluation.png\")\n",
    "    \n",
    "    if os.path.exists(temp_img_path):\n",
    "        os.unlink(temp_img_path)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not save visualization to stage: {e}\")\n",
    "\n",
    "print(\"âœ… Visualizations created successfully\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"SAVING MODELS AND METRICS TO SNOWFLAKE STAGE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "try:\n",
    "    stage_name = \"fe_features.model_stage\"\n",
    "    session.sql(f\"CREATE OR REPLACE STAGE {stage_name}\").collect()\n",
    "    print(f\"âœ… Stage created: {stage_name}\")\n",
    "    \n",
    "    lr_model_path = None\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pkl\") as f:\n",
    "            pickle.dump(lr_model, f)\n",
    "            lr_model_path = f.name\n",
    "        \n",
    "        session.file.put(\n",
    "            local_file_name=lr_model_path,\n",
    "            stage_location=f\"@{stage_name}/lr_model.pkl\",\n",
    "            overwrite=True\n",
    "        )\n",
    "        print(f\"âœ… Logistic Regression model saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error saving LR model: {e}\")\n",
    "    finally:\n",
    "        if lr_model_path and os.path.exists(lr_model_path):\n",
    "            try:\n",
    "                os.unlink(lr_model_path)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    rf_model_path = None\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pkl\") as f:\n",
    "            pickle.dump(rf_model, f)\n",
    "            rf_model_path = f.name\n",
    "        \n",
    "        session.file.put(\n",
    "            local_file_name=rf_model_path,\n",
    "            stage_location=f\"@{stage_name}/rf_model.pkl\",\n",
    "            overwrite=True\n",
    "        )\n",
    "        print(f\"âœ… Random Forest model saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error saving RF model: {e}\")\n",
    "    finally:\n",
    "        if rf_model_path and os.path.exists(rf_model_path):\n",
    "            try:\n",
    "                os.unlink(rf_model_path)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    metrics_summary = {\n",
    "        'training_date': str(pd.Timestamp.now()),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features_used': len(feature_columns),\n",
    "        'feature_names': feature_columns,\n",
    "        'logistic_regression': {\n",
    "            'accuracy': float(accuracy_score(y_test, y_pred_lr)),\n",
    "            'precision': float(precision_score(y_test, y_pred_lr, zero_division=0)),\n",
    "            'recall': float(recall_score(y_test, y_pred_lr, zero_division=0)),\n",
    "            'f1_score': float(f1_score(y_test, y_pred_lr, zero_division=0)),\n",
    "            'cv_f1_mean': float(cv_scores_lr.mean()),\n",
    "            'cv_f1_std': float(cv_scores_lr.std())\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'accuracy': float(accuracy_score(y_test, y_pred_rf)),\n",
    "            'precision': float(precision_score(y_test, y_pred_rf, zero_division=0)),\n",
    "            'recall': float(recall_score(y_test, y_pred_rf, zero_division=0)),\n",
    "            'f1_score': float(f1_score(y_test, y_pred_rf, zero_division=0)),\n",
    "            'cv_f1_mean': float(cv_scores_rf.mean()),\n",
    "            'cv_f1_std': float(cv_scores_rf.std())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    metrics_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\", mode='w').name\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics_summary, f, indent=2)\n",
    "    \n",
    "    session.file.put(\n",
    "        local_file_name=metrics_path,\n",
    "        stage_location=f\"@{stage_name}/model_metrics.json\",\n",
    "        overwrite=True\n",
    "    )\n",
    "    print(f\"âœ… Metrics summary saved\")\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        os.unlink(metrics_path)\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Error creating stage: {e}\")\n",
    "    print(\"Model training completed, but models were not saved to stage.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š TRAINING SUMMARY:\")\n",
    "print(f\"   â€¢ Training samples: {len(X_train)}\")\n",
    "print(f\"   â€¢ Test samples: {len(X_test)}\")\n",
    "print(f\"   â€¢ Features used: {len(feature_columns)}\")\n",
    "print(f\"   â€¢ Models trained: Logistic Regression, Random Forest\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ LOGISTIC REGRESSION PERFORMANCE:\")\n",
    "print(f\"   â€¢ Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"   â€¢ Precision: {precision_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ Recall: {recall_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ F1 Score: {f1_score(y_test, y_pred_lr, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ CV F1 Score (mean Â± std): {cv_scores_lr.mean():.4f} Â± {cv_scores_lr.std():.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ RANDOM FOREST PERFORMANCE:\")\n",
    "print(f\"   â€¢ Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"   â€¢ Precision: {precision_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ Recall: {recall_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ F1 Score: {f1_score(y_test, y_pred_rf, zero_division=0):.4f}\")\n",
    "print(f\"   â€¢ CV F1 Score (mean Â± std): {cv_scores_rf.mean():.4f} Â± {cv_scores_rf.std():.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ NEXT STEPS:\")\n",
    "print(\"   1. Run 06_online_inference.py for predictions on new data\")\n",
    "print(\"   2. Run 07_feature_refresh.sql to refresh features regularly\")\n",
    "print(\"   3. Models available as 'lr_model' and 'rf_model' for this session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1863ea0-ca2b-4764-9a72-e1ea5116a8f7",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 6: Online Feature Retrieval for Inference\n",
    "# Run this in a Snowflake Python Worksheet\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "try:\n",
    "    _ = session\n",
    "    print(\"âœ… Using session from notebook context\")\n",
    "except NameError:\n",
    "    try:\n",
    "        import snowflake.snowpark.context as snowpark_context\n",
    "        session = snowpark_context.get_active_session()\n",
    "        print(\"âœ… Got session from snowpark context\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Could not obtain Snowflake session. Are you running in a Snowflake notebook? ({e})\"\n",
    "        )\n",
    "\n",
    "session.use_warehouse(\"FE_WH\")\n",
    "session.use_database(\"FE_DEMO_DB\")\n",
    "session.use_schema(\"FE_FEATURES\")\n",
    "\n",
    "feature_columns = [\n",
    "    'days_since_last_event',\n",
    "    'purchases_7d',\n",
    "    'revenue_7d',\n",
    "    'refunds_7d',\n",
    "    'refund_amount_7d',\n",
    "    'events_7d',\n",
    "    'mobile_events_7d',\n",
    "    'web_events_7d',\n",
    "    'avg_amount_nonzero_7d',\n",
    "    'browse_events_7d',\n",
    "    'amount_stddev_7d',\n",
    "    'max_amount_7d',\n",
    "    'min_amount_7d'\n",
    "]\n",
    "\n",
    "def get_customer_features(customer_id: str):\n",
    "    \"\"\"\n",
    "    Retrieve latest features for a customer (online inference)\n",
    "    \"\"\"\n",
    "    features_df = session.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM fe_store.customer_features_online\n",
    "        WHERE customer_id = '{customer_id}'\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    features_df.columns = [c.lower() for c in features_df.columns]\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def load_model_from_stage(model_name='rf_model.pkl'):\n",
    "    \"\"\"Load the saved model from Snowflake stage\"\"\"\n",
    "    stage_name = \"fe_features.model_stage\"\n",
    "    \n",
    "    try:\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        \n",
    "        session.file.get(\n",
    "            stage_location=f\"@{stage_name}/{model_name}\",\n",
    "            target_directory=temp_dir\n",
    "        )\n",
    "        \n",
    "        local_path = os.path.join(temp_dir, model_name)\n",
    "        \n",
    "        with open(local_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        os.unlink(local_path)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load {model_name} from stage: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_churn_batch(customer_ids: list, model=None):\n",
    "    \"\"\"\n",
    "    Predict churn probability for multiple customers\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = load_model_from_stage('rf_model.pkl')\n",
    "        if model is None:\n",
    "            print(\"âš ï¸  Model not found in stage. Loading from training set...\")\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            training_df = session.sql(\"SELECT * FROM fe_features.training_set\").to_pandas()\n",
    "            training_df.columns = [c.lower() for c in training_df.columns]\n",
    "            X_train = training_df[feature_columns].fillna(0)\n",
    "            y_train = training_df['label'].astype(int)\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "            model.fit(X_train, y_train)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for customer_id in customer_ids:\n",
    "        features_df = get_customer_features(customer_id)\n",
    "        \n",
    "        if features_df.empty:\n",
    "            results.append({\n",
    "                'customer_id': customer_id,\n",
    "                'status': 'NOT_FOUND',\n",
    "                'churn_probability': None,\n",
    "                'churn_prediction': None\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            X_inference = features_df[feature_columns].fillna(0)\n",
    "            \n",
    "            churn_probability = model.predict_proba(X_inference)[0, 1]\n",
    "            churn_prediction = model.predict(X_inference)[0]\n",
    "            \n",
    "            results.append({\n",
    "                'customer_id': customer_id,\n",
    "                'status': 'SUCCESS',\n",
    "                'churn_probability': float(churn_probability),\n",
    "                'churn_prediction': bool(churn_prediction),\n",
    "                'key_features': {\n",
    "                    'purchases_7d': float(features_df['purchases_7d'].iloc[0]),\n",
    "                    'revenue_7d': float(features_df['revenue_7d'].iloc[0]),\n",
    "                    'days_since_last_event': float(features_df['days_since_last_event'].iloc[0]),\n",
    "                    'events_7d': float(features_df['events_7d'].iloc[0])\n",
    "                }\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'customer_id': customer_id,\n",
    "                'status': 'ERROR',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ONLINE FEATURE RETRIEVAL AND INFERENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- Loading all customers for batch prediction ---\")\n",
    "all_customers_df = session.sql(\"\"\"\n",
    "    SELECT DISTINCT customer_id \n",
    "    FROM fe_features.training_set \n",
    "    ORDER BY customer_id\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "all_customers_df.columns = [c.lower() for c in all_customers_df.columns]\n",
    "\n",
    "all_customer_ids = all_customers_df['customer_id'].tolist()\n",
    "print(f\"Found {len(all_customer_ids)} customers for prediction\")\n",
    "\n",
    "print(\"\\n--- Running Batch Churn Prediction ---\")\n",
    "predictions = predict_churn_batch(all_customer_ids)\n",
    "\n",
    "print(\"\\nðŸ“Š CHURN PREDICTION RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "churn_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Customer ID': p['customer_id'],\n",
    "        'Status': p['status'],\n",
    "        'Churn Probability': f\"{p['churn_probability']:.4f}\" if p.get('churn_probability') else 'N/A',\n",
    "        'Prediction': 'CHURN RISK' if p.get('churn_prediction') else 'RETAIN',\n",
    "        'Purchases (7d)': f\"{p['key_features']['purchases_7d']:.0f}\" if p.get('key_features') else 'N/A',\n",
    "        'Revenue (7d)': f\"${p['key_features']['revenue_7d']:.2f}\" if p.get('key_features') else 'N/A'\n",
    "    }\n",
    "    for p in predictions if p['status'] == 'SUCCESS'\n",
    "])\n",
    "\n",
    "print(churn_results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Saving predictions to Snowflake ---\")\n",
    "try:\n",
    "    predictions_for_sf = []\n",
    "    for p in predictions:\n",
    "        if p['status'] == 'SUCCESS':\n",
    "            predictions_for_sf.append({\n",
    "                'CUSTOMER_ID': p['customer_id'],\n",
    "                'CHURN_PROBABILITY': p['churn_probability'],\n",
    "                'CHURN_PREDICTION': p['churn_prediction'],\n",
    "                'PREDICTION_TIMESTAMP': pd.Timestamp.now(),\n",
    "                'PURCHASES_7D': p['key_features']['purchases_7d'],\n",
    "                'REVENUE_7D': p['key_features']['revenue_7d']\n",
    "            })\n",
    "    \n",
    "    if predictions_for_sf:\n",
    "        predictions_pd = pd.DataFrame(predictions_for_sf)\n",
    "        \n",
    "        session.sql(\"CREATE OR REPLACE TABLE fe_features.inference_results (customer_id STRING, churn_probability FLOAT, churn_prediction BOOLEAN, prediction_timestamp TIMESTAMP, purchases_7d FLOAT, revenue_7d FLOAT)\").collect()\n",
    "        \n",
    "        session.write_pandas(\n",
    "            predictions_pd,\n",
    "            \"INFERENCE_RESULTS\",\n",
    "            database=\"FE_DEMO_DB\",\n",
    "            schema=\"FE_FEATURES\",\n",
    "            auto_create_table=True,\n",
    "            overwrite=True\n",
    "        )\n",
    "        print(f\"âœ… Saved {len(predictions_for_sf)} predictions to fe_features.inference_results\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not save predictions to table: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ INFERENCE SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "successful_predictions = [p for p in predictions if p['status'] == 'SUCCESS']\n",
    "if successful_predictions:\n",
    "    probabilities = [p['churn_probability'] for p in successful_predictions]\n",
    "    churn_predictions = [p['churn_prediction'] for p in successful_predictions]\n",
    "    \n",
    "    print(f\"Total predictions: {len(successful_predictions)}\")\n",
    "    print(f\"Customers at churn risk: {sum(churn_predictions)} ({100*sum(churn_predictions)/len(churn_predictions):.1f}%)\")\n",
    "    print(f\"Average churn probability: {sum(probabilities)/len(probabilities):.4f}\")\n",
    "    print(f\"Min churn probability: {min(probabilities):.4f}\")\n",
    "    print(f\"Max churn probability: {max(probabilities):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… INFERENCE COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6fce0-57ff-49e1-ad4e-067eb25b75b0",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "-- ============================================================================\n",
    "-- PHASE 7: Feature Refresh and Maintenance\n",
    "-- Run this periodically to update features (daily/hourly recommended)\n",
    "-- ============================================================================\n",
    "\n",
    "USE WAREHOUSE fe_wh;\n",
    "USE DATABASE fe_demo_db;\n",
    "USE SCHEMA fe_features;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS fe_features.feature_refresh_log (\n",
    "    refresh_id STRING DEFAULT UUID_STRING(),\n",
    "    refresh_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "    records_updated NUMBER,\n",
    "    status STRING,\n",
    "    error_message STRING\n",
    ");\n",
    "\n",
    "DELETE FROM fe_features.customer_features;\n",
    "\n",
    "INSERT INTO fe_features.customer_features\n",
    "SELECT \n",
    "  *,\n",
    "  CURRENT_TIMESTAMP() AS feature_timestamp\n",
    "FROM fe_features.customer_features_v;\n",
    "\n",
    "INSERT INTO fe_features.feature_refresh_log (records_updated, status)\n",
    "SELECT \n",
    "  COUNT(*) AS records_updated,\n",
    "  'SUCCESS' AS status\n",
    "FROM fe_features.customer_features\n",
    "WHERE feature_timestamp >= DATEADD('minute', -5, CURRENT_TIMESTAMP());\n",
    "\n",
    "CREATE OR REPLACE TABLE fe_store.customer_features_online AS\n",
    "SELECT *\n",
    "FROM fe_features.customer_features\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY feature_timestamp DESC) = 1;\n",
    "\n",
    "SELECT 'Features refreshed successfully!' AS status, CURRENT_TIMESTAMP() AS refresh_time;\n",
    "\n",
    "SELECT \n",
    "    'Raw Events' AS data_type,\n",
    "    COUNT(*) AS record_count,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "    MIN(event_ts) AS earliest_event,\n",
    "    MAX(event_ts) AS latest_event\n",
    "FROM fe_raw.customer_events\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Engineered Features' AS data_type,\n",
    "    COUNT(*) AS record_count,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "    MIN(feature_timestamp) AS earliest_event,\n",
    "    MAX(feature_timestamp) AS latest_event\n",
    "FROM fe_features.customer_features\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Online Features' AS data_type,\n",
    "    COUNT(*) AS record_count,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "    MIN(feature_timestamp) AS earliest_event,\n",
    "    MAX(feature_timestamp) AS latest_event\n",
    "FROM fe_store.customer_features_online\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Training Labels' AS data_type,\n",
    "    COUNT(*) AS record_count,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "    NULL AS earliest_event,\n",
    "    NULL AS latest_event\n",
    "FROM fe_raw.labels;\n",
    "\n",
    "SELECT TOP 10\n",
    "    refresh_id,\n",
    "    refresh_timestamp,\n",
    "    records_updated,\n",
    "    status,\n",
    "    error_message\n",
    "FROM fe_features.feature_refresh_log\n",
    "ORDER BY refresh_timestamp DESC;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "rprovps@gmail.com",
   "authorId": "4360068034039",
   "authorName": "BIMALPRO77",
   "lastEditTime": 1765364744802,
   "notebookId": "x4lmgonjz336kimysh7l",
   "sessionId": "7dd60f30-8670-433c-bdc5-b4add1f03d0b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
